# Implementation Plan: Claude AI Integration at BYU CS Department

**Target Launch:** Fall 2026 (August 2026)
**Target Courses:** CS 260 (Web Programming), CS 340 (Software Design), CS 329 (QA and DevOps)
**Prepared:** January 2026

## Executive Summary

This implementation plan outlines the timeline and key milestones for integrating Claude AI into three BYU Computer Science courses by Fall 2026. The plan emphasizes faculty preparation, autograder enhancement, clear student policies, and measurable outcomes.

**Critical Success Factors:**
1. Faculty buy-in and training completed by August 2026
2. Autograders enhanced to assess understanding, not just correctness
3. Clear AI usage policies established and communicated to students
4. Budget allocated for Claude licenses and faculty development
5. Assessment framework ready to measure learning outcomes

## Timeline Overview

```
Jan-Apr 2026: Planning & Preparation
May-Aug 2026: Faculty Training & Course Redesign
Aug-Dec 2026: Fall Semester Launch & Support
Dec 2026:     Assessment & Planning for Expansion
```

## Phase 1: Planning & Preparation (January - April 2026)

**Objective:** Establish foundation, secure resources, and align stakeholders

### January 2026
- **Milestone:** Department approval of recommendation and implementation plan
- **Action:** Present recommendation to department leadership and faculty
- **Action:** Identify faculty leads for CS 260, CS 340, CS 329
- **Action:** Form AI Integration Working Group (faculty + instructional design + TA coordinator)
- **Deliverable:** Approved plan with identified course leads

### February 2026
- **Milestone:** Licenses and technical infrastructure secured
- **Action:** Procure Claude Team/Enterprise licenses for students (estimated 200-300 seats)
- **Action:** Determine access model (department licenses vs. student subscriptions)
- **Action:** Set up administrative access for faculty and TAs
- **Action:** Coordinate with BYU IT on account provisioning
- **Deliverable:** Claude access ready for faculty and pilot students

### March 2026
- **Milestone:** AI Usage Policy Framework approved
- **Action:** Draft AI usage policy for target courses (3-tier framework: encouraged/allowed/prohibited)
- **Action:** Work with honor code office on academic integrity guidelines
- **Action:** Create template syllabi language for AI tool policies
- **Action:** Review and align with any university-wide AI policies
- **Deliverable:** Approved AI usage policy document for courses

### April 2026
- **Milestone:** Autograding enhancement plan finalized
- **Action:** Audit existing autograders for CS 260, CS 340, CS 329
- **Action:** Identify which assignments need redesign for multi-dimensional assessment
- **Action:** Develop specifications for enhanced autograders (design quality, explanation, testing)
- **Action:** Determine if Claude API will be used for automated explanation assessment
- **Action:** Allocate development resources (staff, TAs, or external contractors)
- **Deliverable:** Autograder enhancement roadmap with assigned owners

## Phase 2: Faculty Training & Course Redesign (May - August 2026)

**Objective:** Prepare faculty and courses for successful AI integration

### May 2026
- **Milestone:** Faculty workshop series begins
- **Action:** Workshop 1 - "Effective Use of Claude for Teaching" (hands-on with Claude capabilities)
- **Action:** Workshop 2 - "Designing AI-Era Assignments" (autograding for understanding, constraint-based assessment)
- **Action:** Workshop 3 - "Academic Integrity in the AI Era" (honor code integration, detecting vs. embracing AI use)
- **Action:** Provide course redesign stipends or release time for participating faculty
- **Deliverable:** Faculty trained on Claude pedagogy and assessment strategies

### June 2026
- **Milestone:** Course materials redesigned
- **Action:** CS 260 instructor redesigns assignments with AI-appropriate usage tiers
- **Action:** CS 340 instructor updates design projects with explanation requirements
- **Action:** CS 329 instructor redesigns testing/DevOps assignments for autograding
- **Action:** Develop shared assignment templates and rubrics for AI-era assessment
- **Action:** Create student-facing "How to Use Claude Effectively" guide
- **Deliverable:** Redesigned syllabi and assignments for all three courses

### July 2026
- **Milestone:** Autograders enhanced and tested
- **Action:** Implement multi-dimensional autograding (correctness + design + testing + explanation)
- **Action:** Integrate constraint-based checks (architecture patterns, performance requirements)
- **Action:** Build or integrate LLM-based explanation quality assessment
- **Action:** Test enhanced autograders with sample student submissions
- **Action:** Train TAs on new autograding system and how to support students
- **Deliverable:** Production-ready enhanced autograders for target courses

### August 2026 (Pre-Semester)
- **Milestone:** Launch readiness achieved
- **Action:** Final faculty prep meeting and Q&A
- **Action:** Distribute student guides on Claude usage and academic integrity
- **Action:** Set up course-specific Claude usage guidelines in Learning Management System
- **Action:** Prepare orientation materials for first day of class
- **Action:** Establish weekly faculty check-in schedule for ongoing support
- **Deliverable:** All courses ready for Day 1 of Fall semester

## Phase 3: Fall 2026 Launch & Support (August - December 2026)

**Objective:** Execute successful launch with continuous support and monitoring

### August - September 2026
- **Milestone:** Successful course launch and student onboarding
- **Action:** First-week orientation on AI tools and academic integrity in all three courses
- **Action:** Students receive access to Claude with clear usage guidelines
- **Action:** First assignments use "encouraged" tier to build comfort with AI collaboration
- **Action:** Collect early feedback from students on Claude access and clarity of policies
- **Action:** Weekly faculty working group meetings to share experiences and troubleshoot
- **Deliverable:** Students successfully onboarded to AI-assisted coursework

### October - November 2026
- **Milestone:** Mid-semester assessment and adjustment
- **Action:** Review autograder effectiveness (are they assessing understanding?)
- **Action:** Monitor academic integrity (any concerns or patterns?)
- **Action:** Gather student feedback through mid-semester surveys
- **Action:** Adjust assignment tiers or policies based on early experience
- **Action:** Document successful practices and challenges for future semesters
- **Deliverable:** Mid-semester progress report with adjustments made

### December 2026
- **Milestone:** End-of-semester data collection and analysis
- **Action:** Collect final student surveys on Claude usage and learning impact
- **Action:** Faculty debrief on what worked, what didn't, and what to change
- **Action:** Analyze academic performance data (do students using Claude learn effectively?)
- **Action:** Review academic integrity incidents related to AI use
- **Action:** Compile case studies and examples for sharing with broader faculty
- **Deliverable:** Fall 2026 Final Report with recommendations for expansion

## Phase 4: Assessment & Planning for Expansion (December 2026)

**Objective:** Evaluate pilot success and plan broader rollout

### December 2026 - January 2027
- **Milestone:** Pilot evaluation complete
- **Action:** Analyze student learning outcomes (performance on AI-assisted vs. independent assessments)
- **Action:** Evaluate faculty satisfaction and readiness to continue
- **Action:** Assess autograding effectiveness at scale
- **Action:** Calculate ROI (student outcomes improvement vs. license and development costs)
- **Action:** Present findings to department for decisions on expansion
- **Deliverable:** Comprehensive pilot evaluation report

### Planning for Spring/Fall 2027 Expansion
- **Action:** Identify additional courses for next phase based on pilot success
- **Action:** Refine policies and assignment templates based on lessons learned
- **Action:** Develop faculty training materials for wider adoption
- **Action:** Consider publishing or presenting findings to broader CS education community
- **Deliverable:** Expansion plan for 2027-2028 academic year

## Critical Milestones Summary

| Date | Milestone | Responsible Party |
|------|-----------|-------------------|
| Jan 2026 | Department approval and faculty leads identified | Department Chair + Faculty Leads |
| Feb 2026 | Claude licenses secured and accessible | IT Coordinator + Budget Office |
| Mar 2026 | AI usage policy approved | Faculty Working Group + Honor Code Office |
| Apr 2026 | Autograder enhancement plan finalized | Technical Lead + Course Instructors |
| May 2026 | Faculty training workshops completed | Instructional Design + Faculty Leads |
| Jun 2026 | Course materials redesigned | Course Instructors |
| Jul 2026 | Enhanced autograders tested and ready | Technical Lead + TAs |
| Aug 2026 | Fall semester launch | All faculty and TAs |
| Oct 2026 | Mid-semester assessment complete | Faculty Working Group |
| Dec 2026 | Pilot evaluation and expansion planning | Department Leadership |

## Key Roles and Responsibilities

**Department Chair / Leadership**
- Approve plan and budget
- Communicate vision to faculty and students
- Make decisions on expansion based on pilot results

**Faculty Leads (CS 260, CS 340, CS 329 Instructors)**
- Redesign courses with AI integration
- Implement AI usage policies in their courses
- Provide regular feedback on effectiveness
- Participate in working group

**AI Integration Coordinator** (Recommended: Designate one faculty member)
- Lead faculty workshops
- Coordinate autograder development
- Support faculty during implementation
- Compile assessment data and reports

**Technical Lead / Autograding Specialist**
- Enhance autograding systems
- Integrate LLM-based assessment
- Train TAs on new systems
- Troubleshoot technical issues

**Instructional Design Support**
- Assist faculty with assignment redesign
- Develop student-facing resources
- Create assessment rubrics
- Document best practices

**TAs (Teaching Assistants)**
- Learn enhanced autograding system
- Support students with Claude usage
- Monitor for academic integrity concerns
- Provide feedback on student experience

## Success Metrics

### Student Outcomes
- **Learning effectiveness:** Students demonstrate understanding on both AI-assisted and independent assessments
- **Skill development:** Students can explain design decisions and verify AI-generated code
- **Professional readiness:** Exit surveys show confidence in using AI tools professionally
- **Satisfaction:** 80%+ of students report positive experience with Claude integration

### Faculty Outcomes
- **Confidence:** Faculty feel prepared to teach with AI tools
- **Course quality:** Faculty report ability to cover more advanced material or assign more ambitious projects
- **Sustainability:** Faculty willing to continue AI integration in future semesters
- **Workload:** Faculty workload is manageable with enhanced autograding

### Department Outcomes
- **Academic integrity:** No significant increase in academic integrity violations
- **Scalability:** Autograding successfully handles 1,500 students with 40 faculty
- **Leadership:** BYU CS recognized for innovative AI pedagogy
- **Replicability:** Other departments express interest in adopting approach

### Measurable Targets
- 200-300 students successfully complete courses with Claude integration
- Average student satisfaction rating ≥ 4.0/5.0 on AI integration
- Faculty report ≤10% increase in grading workload despite more sophisticated assessment
- Academic integrity incident rate remains flat or decreases
- 100% of pilot faculty willing to continue in Spring 2027

## Risk Mitigation

### Risk: Students over-rely on AI and don't develop fundamental skills
**Mitigation:**
- Multi-dimensional autograding tests understanding, not just correctness
- Periodic "no AI" assessments to verify independent capability
- Progressive scaffolding (limit AI early, increase later in semester)

### Risk: Autograder development takes longer than expected
**Mitigation:**
- Start autograder work in April with 3-month buffer before launch
- Prioritize most critical enhancements; defer nice-to-haves if needed
- Have manual grading backup plan for subset of assignments

### Risk: Faculty resistance or insufficient training
**Mitigation:**
- Target courses with enthusiastic faculty for pilot (CS 260 instructor eager)
- Provide stipends and course release time for redesign work
- Ongoing support through weekly working group meetings
- Clear documentation and templates to reduce burden

### Risk: Budget constraints or license access issues
**Mitigation:**
- Budget already allocated (confirmed)
- Negotiate educational pricing with Anthropic
- Have free-tier fallback option for students if needed
- Computer lab access for students without personal subscriptions

### Risk: Academic integrity incidents spike
**Mitigation:**
- Clear policies from Day 1 integrated with BYU Honor Code
- Autograders designed to be cheat-resistant (multi-dimensional assessment)
- Regular faculty check-ins to identify and address concerns early
- Student education on appropriate vs. inappropriate AI use

### Risk: University-level policy changes restrict AI use
**Mitigation:**
- Engage with university administration early (March 2026)
- Position as professional skills development, not just convenience
- Document pedagogical rationale and alignment with learning outcomes
- Build relationships with other departments doing AI integration

## Budget Considerations

**Estimated Costs for Fall 2026 Pilot:**

**Claude Licenses**
- 250 students × $30/month (Claude Team) × 4 months = $30,000
- Alternative: Negotiate annual educational license for lower per-student cost

**Faculty Development**
- Summer stipends for 3 faculty course redesign (3 × $3,000) = $9,000
- Workshop facilitation and materials = $2,000

**Autograding Development**
- Technical development (contractor or TA hours) = $10,000-15,000
- Claude API costs for explanation assessment = $1,000-2,000

**Support and Administration**
- AI Integration Coordinator (course release or stipend) = $5,000
- Instructional design support = $3,000

**Total Estimated Budget: $60,000-66,000 for Fall 2026 pilot**

**Notes:**
- Budget already allocated per initial discussions
- Costs decrease significantly in subsequent semesters (autograders built, faculty trained)
- Potential for external funding or educational grants given innovative nature

## Next Steps (Immediate Actions)

1. **Schedule department meeting** to present recommendation and implementation plan (Target: Late January 2026)

2. **Identify specific faculty members** for CS 260, CS 340, CS 329 who will lead pilot

3. **Initiate license procurement process** with Anthropic and BYU procurement office

4. **Form AI Integration Working Group** with representatives from:
   - Faculty leads from target courses
   - Department leadership
   - Instructional design
   - TA coordinator
   - Technical/autograding specialist

5. **Assign AI Integration Coordinator role** to manage implementation

6. **Schedule initial working group meeting** to kick off planning phase (Target: February 2026)

## Appendices

### Appendix A: Sample AI Usage Policy Template
(See Recommendation document Section 2.1 for detailed usage framework)

### Appendix B: Assignment Redesign Checklist
- [ ] Define which tier (encouraged/allowed/prohibited) applies to this assignment
- [ ] Specify learning objectives being assessed
- [ ] Identify what students must demonstrate understanding of independently
- [ ] Design autograder to test multiple dimensions (not just correctness)
- [ ] Create rubric for explanation/documentation requirements
- [ ] Add constraint requirements if applicable (recursion-only, pattern usage, etc.)
- [ ] Write student-facing instructions with clear AI usage guidelines
- [ ] Prepare sample solutions showing good vs. poor AI collaboration

### Appendix C: Student Guide Outline: "Working Effectively with Claude"
1. What Claude can help with (and what it can't)
2. When to use Claude (and when to work independently)
3. How to prompt effectively
4. Verifying and testing AI-generated code
5. Academic integrity and honor code expectations
6. Getting help when Claude isn't sufficient

### Appendix D: Faculty Support Resources
- Weekly working group meetings (Fridays during Fall 2026)
- Shared assignment repository
- Slack channel for quick questions
- Office hours with AI Integration Coordinator
- Documentation wiki for troubleshooting

### Appendix E: Using Claude API for Autograding Design Documents

One of the most powerful applications of AI in education is using Claude itself to grade subjective aspects of student work at scale. This addresses the 1,500:40 student-to-faculty ratio while assessing higher-order thinking.

#### How It Works

**Architecture:**
```
Student submits → Autograder system → Calls Claude API → Returns structured assessment → Recorded in gradebook
```

**Example: Grading a Software Design Document**

```python
import anthropic

def grade_design_document(student_code, design_doc, rubric):
    """
    Use Claude API to assess whether design document demonstrates understanding.
    """
    client = anthropic.Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY"))

    prompt = f"""You are an expert software engineering instructor grading a student's design document.

ASSIGNMENT: Design and implement a task management system

STUDENT'S DESIGN DOCUMENT:
{design_doc}

STUDENT'S IMPLEMENTATION CODE:
{student_code}

GRADING RUBRIC (40 points total):
1. Design Justification (10 pts): Does student explain WHY they chose this architecture/pattern?
2. Tradeoff Analysis (10 pts): Does student discuss alternatives and explain tradeoffs?
3. Design-Code Alignment (10 pts): Does implementation match the documented design?
4. Depth of Understanding (10 pts): Does explanation go beyond surface-level description?

For each criterion:
- Provide a score (0-10)
- Explain what's strong and what's missing
- Identify if explanation seems AI-generated without genuine understanding
- Note specific examples from the document

Return your assessment in this JSON format:
{{
    "design_justification": {{
        "score": <0-10>,
        "feedback": "<specific feedback>",
        "evidence": "<quote from document>"
    }},
    "tradeoff_analysis": {{
        "score": <0-10>,
        "feedback": "<specific feedback>",
        "evidence": "<quote from document>"
    }},
    "design_code_alignment": {{
        "score": <0-10>,
        "feedback": "<specific feedback>",
        "mismatches": ["<any inconsistencies>"]
    }},
    "depth_of_understanding": {{
        "score": <0-10>,
        "feedback": "<specific feedback>",
        "concerns": "<if explanation seems copied/AI-generated without understanding>"
    }},
    "total_score": <sum of all scores>,
    "overall_feedback": "<summary for student>",
    "red_flags": ["<any academic integrity concerns>"]
}}
"""

    message = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=2000,
        messages=[{"role": "user", "content": prompt}]
    )

    # Parse JSON response
    assessment = json.loads(message.content[0].text)
    return assessment
```

#### What Claude Can Assess

**Design Quality Indicators:**
1. **Justification depth:** Does student explain reasoning, or just describe what they did?
2. **Tradeoff awareness:** Does student discuss alternatives and why they chose one approach?
3. **Technical accuracy:** Are design patterns correctly identified and applied?
4. **Alignment with code:** Does implementation match documented design?
5. **Appropriate complexity:** Is solution neither over-engineered nor too simplistic?

**Red Flags for Academic Dishonesty:**
- Generic explanations that could apply to any project
- Missing specific details from their own implementation
- Sophisticated terminology without demonstrating understanding
- Design document doesn't match actual code structure
- Explanation contradicts what code actually does

**Example Assessments:**

**High-quality explanation (9/10):**
> "I chose the Observer pattern because the UI needs to update whenever task status changes. I considered using polling but rejected it due to inefficiency - with 1000+ tasks, constant polling would waste CPU cycles. Observer is more complex to implement but provides real-time updates with minimal overhead. The tradeoff is increased coupling between TaskManager and UI components, which I mitigated by using an event bus interface."

Claude assessment: *Student demonstrates clear understanding of pattern choice, discusses rejected alternatives with technical reasoning, identifies specific tradeoffs, and shows awareness of implementation challenges.*

**Low-quality explanation (3/10):**
> "I used the Observer pattern because it's good for this type of application. It allows objects to communicate efficiently and makes the code more maintainable. This is a widely-used pattern in software engineering."

Claude assessment: *Generic explanation with no specific justification for this project. Doesn't discuss alternatives or tradeoffs. Could be copied from documentation. Student needs to explain WHY Observer fits their specific requirements.*

#### Implementation Strategies

**Calibration Phase:**
1. Have faculty manually grade 20-30 design documents
2. Run same documents through Claude API grading
3. Compare scores and identify discrepancies
4. Refine prompts to align with faculty expectations
5. Establish confidence threshold (e.g., if Claude is < 70% confident, flag for manual review)

**Production Use:**
1. All design documents auto-graded by Claude
2. Scores ≥ 70%: Automated grade recorded, feedback sent to student
3. Scores 50-70%: Automated grade recorded, flagged for spot-check review
4. Scores < 50%: Held for full manual review
5. Random 10% sample manually reviewed for quality control

**Cost Considerations:**
- Claude API cost: ~$0.10-0.30 per design document assessment (depending on document length)
- For 300 students × 5 design assignments = 1,500 assessments = ~$150-450 per semester
- Compared to 40 hours of TA time at $15/hour = $600 saved
- Plus: Faster feedback turnaround for students

#### Benefits

1. **Scalable subjective assessment:** Can grade design quality for 1,500 students
2. **Consistent rubric application:** Claude applies rubric uniformly
3. **Detailed feedback:** Students get specific, actionable feedback on every submission
4. **Academic integrity detection:** Claude can flag suspicious explanations
5. **Faculty time savings:** Instructors review flagged cases only, not all 1,500 submissions

### Appendix F: Cheat-Resistant Assignment Patterns

These assignment types require genuine understanding and cannot be "solved" by simply using Claude to generate code. Students can use Claude as a tool, but must demonstrate learning across multiple dimensions.

#### Pattern 1: Comparative Implementation with Analysis

**Assignment Structure:**
- Implement the same functionality using TWO different approaches
- Write analysis comparing approaches with specific metrics
- Autograder validates both implementations and assesses analysis quality

**Example (CS 340 - Software Design):**
```
Assignment: Task Queue System

Part 1: Implement task queue using:
  - Version A: Array-based circular buffer
  - Version B: Linked list

Part 2: Benchmark both implementations:
  - Measure performance with 1000, 10000, 100000 tasks
  - Compare memory usage
  - Document results in tables/graphs

Part 3: Analysis (submitted as design document, graded by Claude API):
  - When would you choose Version A vs. B in production?
  - Explain time complexity of each operation (enqueue, dequeue, peek)
  - Discuss cache locality and why it matters
  - Recommend which to use for specific scenarios (high throughput, memory-constrained, etc.)

Autograder checks:
  ✓ Both versions implement interface correctly
  ✓ Performance benchmarks actually measure both versions
  ✓ Analysis references student's specific measurements (not generic)
  ✓ Complexity analysis is technically accurate
  ✓ Recommendations are justified by data
```

**Why it's cheat-resistant:**
- Claude can help implement both versions, but student must run benchmarks
- Analysis must reference student's actual performance data
- Generic AI-generated analysis won't match their specific results
- Demonstrates understanding through comparison and tradeoff reasoning

---

#### Pattern 2: Code Archaeology and Refactoring

**Assignment Structure:**
- Provide students with intentionally flawed or legacy code
- Students must identify issues, explain problems, and improve code
- Tests understanding of code review and design principles

**Example (CS 340 - Software Design):**
```
Assignment: Legacy E-commerce System Refactoring

Given: 500-line "legacy" codebase with multiple issues:
  - God object (ShopManager does everything)
  - No error handling
  - Tight coupling (UI directly calls database)
  - No tests
  - Security vulnerabilities (SQL injection)
  - Poor naming and magic numbers

Part 1: Code Review (design document graded by Claude API):
  - Identify at least 5 design problems
  - Explain WHY each is problematic (not just "bad practice")
  - Prioritize issues by severity and impact
  - Propose specific refactoring strategy

Part 2: Implementation:
  - Refactor to improve design (separation of concerns, dependency injection, etc.)
  - Add comprehensive tests
  - Fix security issues
  - Preserve all original functionality

Autograder checks:
  ✓ All original features still work (functional correctness)
  ✓ Code now follows SOLID principles (architectural checks)
  ✓ Tests cover critical paths (test quality assessment)
  ✓ Security vulnerabilities fixed (static analysis)
  ✓ Code review document demonstrates understanding of problems
```

**Why it's cheat-resistant:**
- Claude can help refactor, but student must identify and explain problems first
- Requires reading and understanding existing code, not just generating new code
- Explanation must reference specific lines from the provided codebase
- Tests critical thinking about design quality, not just implementation

---

#### Pattern 3: Constrained Implementation

**Assignment Structure:**
- Implement solution with specific constraints that test understanding
- Constraints chosen to prevent simple AI delegation

**Example (CS 260 - Web Programming):**
```
Assignment: Build a Single Page App WITHOUT Using Frameworks

Constraints:
  - No React, Vue, Angular, or other frameworks
  - Must use vanilla JavaScript only
  - Implement your own component system with state management
  - Implement your own router for multiple views
  - Must handle browser back/forward buttons correctly

Required Features:
  - Multiple views (list, detail, create)
  - Client-side routing
  - State persistence (localStorage)
  - Form validation

Part 1: Architecture Document (Claude-graded):
  - Explain your component architecture
  - How does state management work?
  - How does routing work?
  - Why are these patterns necessary? (demonstrates understanding of what frameworks do)

Part 2: Implementation

Autograder checks:
  ✓ No framework libraries imported (dependency check)
  ✓ Custom component system implemented
  ✓ Routing works without page refresh
  ✓ Back button functionality works
  ✓ Explanation demonstrates understanding of framework internals
```

**Why it's cheat-resistant:**
- Claude can help implement, but constraints force deep engagement with fundamentals
- Student must understand what frameworks do "under the hood"
- Explanation required to articulate why patterns are necessary
- Cannot just accept AI-generated framework boilerplate

---

#### Pattern 4: Test-First Development with Quality Requirements

**Assignment Structure:**
- Require comprehensive tests BEFORE implementation
- Grade test quality, not just code correctness
- Tests reveal understanding of requirements and edge cases

**Example (CS 329 - QA and DevOps):**
```
Assignment: API Testing and Quality Assurance

Part 1: Write comprehensive test suite for [provided API spec]
  - Unit tests for business logic
  - Integration tests for API endpoints
  - Edge cases and error conditions
  - Performance tests
  - Security tests (authentication, authorization, input validation)

Part 2: Justify test strategy (Claude-graded document):
  - Why did you choose these test cases?
  - What edge cases are you testing and why?
  - How did you achieve X% coverage?
  - What scenarios are NOT tested and why?

Part 3: Implement API to pass your tests

Autograder checks:
  ✓ Test coverage ≥ 85%
  ✓ Tests include edge cases (empty inputs, null values, boundary conditions)
  ✓ Tests actually fail when code is wrong (mutation testing)
  ✓ Performance tests validate response time requirements
  ✓ Security tests check for common vulnerabilities
  ✓ Explanation demonstrates testing strategy, not just description
```

**Why it's cheat-resistant:**
- Good tests require understanding requirements deeply
- Claude can help write tests, but student must decide WHAT to test
- Test quality reveals understanding of edge cases and failure modes
- Justification document prevents blind acceptance of AI-generated tests

---

#### Pattern 5: Design-First with Peer Review

**Assignment Structure:**
- Multi-stage assignment with design approval before implementation
- Peer review component requires critical evaluation skills

**Example (CS 340 - Software Design):**
```
Assignment: Multi-Stage Project with Design Gates

Stage 1: Design Proposal (Week 1)
  - Submit architecture diagram and design document
  - Explain pattern choices and justify architecture
  - Faculty/Claude provide feedback
  - Must be approved before proceeding

Stage 2: Implementation (Weeks 2-3)
  - Implement according to approved design
  - Claude assistance allowed for implementation
  - Must match approved architecture

Stage 3: Peer Code Review (Week 4)
  - Review 2 other students' implementations
  - Evaluate: Does code match their design? Is design well-executed?
  - Provide constructive feedback

Stage 4: Revision (Week 5)
  - Address peer feedback
  - Justify which feedback you incorporated and why

Autograder checks:
  ✓ Implementation matches approved design
  ✓ Peer reviews demonstrate critical evaluation (Claude-graded)
  ✓ Revision justification shows reasoning about feedback
```

**Why it's cheat-resistant:**
- Design must be defended before implementation
- Cannot just generate and submit code
- Peer review requires critical thinking about others' work
- Revision justification reveals understanding of design decisions

---

#### Pattern 6: Debugging and Root Cause Analysis

**Assignment Structure:**
- Provide buggy code and failing tests
- Students must diagnose, explain, and fix

**Example (CS 260 - Web Programming):**
```
Assignment: Debug a Broken Web Application

Given: Web app with 5 subtle bugs:
  1. Race condition in async code
  2. Memory leak in event listeners
  3. Incorrect state update causing UI bug
  4. CORS misconfiguration
  5. Off-by-one error in pagination

Part 1: Root Cause Analysis (Claude-graded):
  - For each bug, explain:
    * What is the symptom?
    * What is the root cause?
    * Why does this happen? (technical explanation)
    * How did you diagnose it?

Part 2: Fix Implementation:
  - Fix all bugs
  - Add tests to prevent regression
  - Document prevention strategies

Autograder checks:
  ✓ All bugs fixed (functional correctness)
  ✓ Regression tests added
  ✓ Root cause analysis demonstrates debugging understanding
  ✓ Explanations are technically accurate
```

**Why it's cheat-resistant:**
- Requires reading and understanding existing code
- Claude can suggest fixes, but student must explain diagnosis process
- Root cause analysis reveals debugging thought process
- Cannot just generate new code - must understand what's broken

---

#### Pattern 7: Performance Optimization with Profiling

**Assignment Structure:**
- Provide inefficient but correct code
- Require optimization with measurable improvement
- Must justify optimizations with profiling data

**Example (CS 340 - Software Design):**
```
Assignment: Optimize a Slow System

Given: Correct but inefficient implementation (O(n²) where O(n log n) possible)

Part 1: Profiling and Analysis:
  - Profile code to identify bottlenecks
  - Measure baseline performance
  - Identify algorithmic complexity issues
  - Propose optimization strategy

Part 2: Optimization Implementation:
  - Implement optimizations
  - Measure performance improvement
  - Achieve ≥ 10x speedup on large inputs

Part 3: Justification (Claude-graded):
  - Explain what was slow and why
  - Explain optimization technique and why it works
  - Show profiling data before/after
  - Discuss space/time tradeoffs

Autograder checks:
  ✓ Correctness maintained
  ✓ Performance improvement verified (benchmarks)
  ✓ Profiling data included
  ✓ Explanation demonstrates algorithmic understanding
```

**Why it's cheat-resistant:**
- Must run profiling tools and collect actual data
- Explanation must reference student's specific profiling results
- Performance improvement must be measured, not claimed
- Demonstrates understanding of algorithmic complexity

---

#### Common Patterns Across Cheat-Resistant Assignments

1. **Multi-dimensional requirements:** Code + explanation + data + analysis
2. **Require student-specific artifacts:** Profiling data, benchmark results, specific code references
3. **Justification of decisions:** Not just what, but why and what alternatives were considered
4. **Process documentation:** Show your work, not just final answer
5. **Critical evaluation:** Review, compare, or debug rather than just generate
6. **Constraint-based:** Require specific approaches that test understanding

**Key Principle:** Design assignments where Claude is a useful tool but cannot replace the thinking, analysis, and decision-making that demonstrates learning.

## Conclusion

This implementation plan provides a structured, realistic path to successfully integrating Claude AI into BYU Computer Science courses by Fall 2026. The phased approach with clear milestones, responsible parties, and risk mitigation strategies positions the department for a successful pilot that can expand to serve more students in subsequent semesters.

**Key to success:** Faculty preparation, enhanced autograding, clear policies, and continuous support throughout the Fall 2026 semester.

For questions or to discuss this plan, contact: [AI Integration Coordinator - TBD]
